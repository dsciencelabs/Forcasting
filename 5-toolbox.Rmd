---
title: "ETC3550/ETC5550 Applied&nbsp;forecasting"
author: "Ch5. The forecasters' toolbox"
date: "OTexts.org/fpp3/"
toc: true
colortheme: monashwhite
output:
  binb::monash:
    fig_width: 7
    fig_height: 3.5
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE,
  dev.args = list(pointsize = 11)
)
options(digits = 3, width = 60)
library(fpp3)
source("nicefigs.R")
```

# A tidy forecasting workflow

## A tidy forecasting workflow

The process of producing forecasts can be split up into a few fundamental steps.

1. Preparing data
2. Data visualisation
3. Specifying a model
4. Model estimation
5. Accuracy \& performance evaluation
6. Producing forecasts

## A tidy forecasting workflow

```{r workflow, echo = FALSE}
line_curve <- function(x, y, xend, yend, ...){
  geom_curve(
    aes(x = x, y = y, xend = xend, yend = yend),
    arrow = arrow(type = "closed", length = unit(0.03, "npc")),
    ...
  )
}

ggplot() +
  geom_text(
    aes(x = x, y = y, label = label),
    data = tribble(
      ~ x, ~ y, ~ label,
      1, 0, "Tidy",
      7/3, 0, "Visualise",
      3, 0.5, "Specify",
      11/3, 0, "Estimate",
      3, -0.5, "Evaluate",
      5, 0, "Forecast"
    ),
    size = 5
  ) +
  geom_segment(
    aes(x = x, y = y, xend = xend, yend = yend),
    data = tribble(
      ~ x, ~ y, ~ xend, ~ yend,
      1.3, 0, 1.9, 0,
      4.1, 0, 4.6, 0
    ),
    arrow = arrow(type = "closed", length = unit(0.03, "npc"))
  ) +
  line_curve(7/3, 0.1, 8/3, 0.5, angle = 250, curvature = -0.3) +
  line_curve(10/3, 0.5, 11/3, 0.1, angle = 250, curvature = -0.3) +
  line_curve(8/3, -0.5, 7/3, -0.1, angle = 250, curvature = -0.3) +
  line_curve(11/3, -0.1, 10/3, -0.5, angle = 250, curvature = -0.3) +
  theme_void() +
  xlim(0.8, 5.2) +
  ylim(-0.6, 0.6) +
  coord_equal(ratio = 1)
```

## Data preparation (tidy)
\fontsize{10}{13}\sf

```{r GDPpc, fig.height = 3.2}
gdppc <- global_economy %>%
  mutate(GDP_per_capita = GDP/Population) %>%
  select(Year, Country, GDP, Population, GDP_per_capita)
gdppc
```


## Data visualisation
\fontsize{10}{13}\sf

```{r GDP-plot, fig.height = 3.2}
gdppc %>%
  filter(Country=="Sweden") %>%
  autoplot(GDP_per_capita) +
    labs(title = "GDP per capita for Sweden", y = "$US")
```

## Model estimation

The `model()` function trains models to data.

\fontsize{10}{13}\sf

```{r GDP-model, warning=FALSE}
fit <- gdppc %>%
  model(trend_model = TSLM(GDP_per_capita ~ trend()))
fit
```

\only<2>{\begin{textblock}{9.5}(1,8.6)
\begin{alertblock}{}
A \texttt{mable} is a model table, each cell corresponds to a fitted model.
\end{alertblock}
\end{textblock}}

## Producing forecasts

\fontsize{10}{13}\sf

```{r GDP-fc, echo = TRUE, dependson='GDP-model', warning=FALSE}
fit %>% forecast(h = "3 years")
```


\only<2>{\begin{textblock}{9.5}(1,8.6)
\begin{alertblock}{}
A \texttt{fable} is a forecast table with point forecasts and distributions.
\end{alertblock}
\end{textblock}}


## Visualising forecasts

\footnotesize

```{r GDP-fc-plot, warning=FALSE, message=FALSE, fig.height=3}
fit %>% forecast(h = "3 years") %>%
  filter(Country=="Sweden") %>%
  autoplot(gdppc) +
    labs(title = "GDP per capita for Sweden", y = "$US")
```

# Some simple forecasting methods

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `MEAN(y)`: Average method

  * Forecast of all future values is equal to mean of historical data $\{y_1,\dots,y_T\}$.
  * Forecasts: $\hat{y}_{T+h|T} = \bar{y} = (y_1+\dots+y_T)/T$

```{r mean-method-explained, echo=FALSE, message=FALSE, warning=FALSE, fig.height = 3.3}
bricks <- aus_production %>%
  filter(!is.na(Bricks)) %>%
  mutate(average = mean(Bricks))

fc <- bricks %>%
  filter(row_number() == n()) %>% as_tibble() %>%
  unnest(Quarter = list(as.Date(Quarter) + months(c(0, 12*5))))

bricks %>%
  ggplot(aes(x = Quarter, y = Bricks)) +
  geom_line() +
  geom_line(aes(y = average), colour = "#0072B2", linetype = "dashed") +
  geom_line(aes(y = average), data = fc, colour = "#0072B2") +
  labs(title = "Clay brick production in Australia")
```

\vspace*{10cm}

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `NAIVE(y)`: Naïve method

  * Forecasts equal to last observed value.
  * Forecasts: $\hat{y}_{T+h|T} =y_T$.
  * Consequence of efficient market hypothesis.

```{r naive-method-explained, echo = FALSE, warning = FALSE, fig.height = 3.1}
bricks %>%
  filter(!is.na(Bricks)) %>%
  model(NAIVE(Bricks)) %>%
  forecast(h = "5 years") %>%
  autoplot(filter(bricks, year(Quarter) > 1990), level = NULL) +
  geom_point(data = slice(bricks, n()), aes(y=Bricks), colour = "#0072B2") +
  labs(title = "Clay brick production in Australia")
```

\vspace*{10cm}

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `SNAIVE(y ~ lag(m))`: Seasonal naïve method

  * Forecasts equal to last value from same season.
  * Forecasts: $\hat{y}_{T+h|T} =y_{T+h-m(k+1)}$, where $m=$ seasonal period and $k$ is the integer part of $(h-1)/m$.

```{r snaive-method-explained, echo = FALSE, warning = FALSE, fig.height = 2.8}
bricks %>%
  model(SNAIVE(Bricks ~ lag("year"))) %>%
  forecast(h = "5 years") %>%
  autoplot(filter(bricks, year(Quarter) > 1990), level = NULL) +
  geom_point(data = slice(bricks, (n()-3):n()), aes(y=Bricks), colour = "#0072B2") +
  labs(title = "Clay brick production in Australia")
```

\vspace*{10cm}

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `RW(y ~ drift())`: Drift method

 * Forecasts equal to last value plus average change.
 * Forecasts:\vspace*{-.7cm}

 \begin{align*}
 \hat{y}_{T+h|T} & =  y_{T} + \frac{h}{T-1}\sum_{t=2}^T (y_t-y_{t-1})\\
                 & = y_T + \frac{h}{T-1}(y_T -y_1).
 \end{align*}\vspace*{-0.2cm}

   * Equivalent to extrapolating a line drawn between first and last observations.

## Some simple forecasting methods

### Drift method

```{r drift-method-explained, echo = FALSE, warning = FALSE}
aus_production %>%
  filter(!is.na(Bricks)) %>%
  model(RW(Bricks ~ drift())) %>%
  forecast(h = "5 years") %>%
  autoplot(aus_production, level = NULL) +
  geom_line(data = slice(aus_production, range(cumsum(!is.na(Bricks)))),
            aes(y=Bricks), linetype = "dashed", colour = "#0072B2") +
  labs(title = "Clay brick production in Australia")
```

\vspace*{10cm}

## Model fitting

The `model()` function trains models to data.

\fontsize{10}{11}\sf

```{r brick-model}
brick_fit <-  aus_production %>%
  filter(!is.na(Bricks)) %>%
  model(
    Seasonal_naive = SNAIVE(Bricks),
    Naive = NAIVE(Bricks),
    Drift = RW(Bricks ~ drift()),
    Mean = MEAN(Bricks)
  )
```

```{r brick-model2, echo=FALSE, dependson='brick-model'}
brick_fit
```


## Producing forecasts

\fontsize{10}{13}\sf

```{r brick-fc, echo = TRUE, dependson='brick-model'}
brick_fc <- brick_fit %>%
  forecast(h = "5 years")
```

```{r brick-fbl, echo = FALSE, dependson='brick-fc'}
print(brick_fc, n = 4)
```

## Visualising forecasts

\footnotesize

```{r brick-fc-plot, warning=FALSE, message=FALSE, fig.height=3, dependson='brick-fc'}
brick_fc %>%
  autoplot(aus_production, level = NULL) +
  labs(title = "Clay brick production in Australia",
       y = "Millions of bricks") +
  guides(colour = guide_legend(title = "Forecast"))
```

## Facebook closing stock price

\fontsize{9}{10}\sf

```{r fbf2, fig.show='hide'}
# Extract training data
fb_stock <- gafa_stock %>%
  filter(Symbol == "FB") %>%
  mutate(trading_day = row_number()) %>%
  update_tsibble(index=trading_day, regular=TRUE)

# Specify, estimate and forecast
fb_stock %>%
  model(
    Mean = MEAN(Close),
    Naive = NAIVE(Close),
    Drift = RW(Close ~ drift())
  ) %>%
  forecast(h=42) %>%
  autoplot(fb_stock, level = NULL) +
  labs(title = "Facebook closing stock price", y="$US") +
  guides(colour=guide_legend(title="Forecast"))
```

## Facebook closing stock price

```{r ref.label='fbf2', echo=FALSE, fig.height=4.6}
```

<!--
## Your turn

 * Produce forecasts using an appropriate benchmark method for household wealth (`hh_budget`). Plot the results using `autoplot()`.
 * Produce forecasts using an appropriate benchmark method for Australian takeaway food turnover (`aus_retail`). Plot the results using `autoplot()`.
 -->

# Residual diagnostics

## Fitted values

 - $\hat{y}_{t|t-1}$ is the forecast of $y_t$ based on observations $y_1,\dots,y_{t-1}$.
 - We call these "fitted values".
 - Sometimes drop the subscript: $\hat{y}_t \equiv \hat{y}_{t|t-1}$.
 - Often not true forecasts since parameters are estimated on all data.

### For example:

 - $\hat{y}_{t} = \bar{y}$ for average method.
 - $\hat{y}_{t} = y_{t-1} + (y_{T}-y_1)/(T-1)$ for drift method.

## Forecasting residuals

\begin{block}{}
\textbf{Residuals in forecasting:} difference between observed value and its fitted value: $e_t = y_t-\hat{y}_{t|t-1}$.
\end{block}
\pause\fontsize{13}{15}\sf

\alert{Assumptions}

  1. $\{e_t\}$ uncorrelated. If they aren't, then information left in  residuals that should be used in computing forecasts.
  2. $\{e_t\}$ have mean zero. If they don't, then forecasts are biased.

\pause

\alert{Useful properties} (for distributions & prediction intervals)

  3. $\{e_t\}$ have constant variance.
  4. $\{e_t\}$ are normally distributed.

## Facebook closing stock price
\fontsize{9}{10}\sf

```{r fbf, fig.height=3.5}
fb_stock <- gafa_stock %>%
  filter(Symbol == "FB") %>%
  mutate(trading_day = row_number()) %>%
  update_tsibble(index = trading_day, regular = TRUE)
fb_stock %>% autoplot(Close)
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r augment}
fit <- fb_stock %>% model(NAIVE(Close))
augment(fit)
```

\only<2>{\begin{textblock}{6}(.5,6.9)\fontsize{14}{16}\sf
\begin{alertblock}{Na\"{\i}ve forecasts:}\vspace*{-0.4cm}
\begin{align*}
\hat{y}_{t|t-1} & = y_{t-1}\\
e_t & = y_t - \hat{y}_{t|t-1} = y_t-y_{t-1}
\end{align*}
\end{alertblock}\end{textblock}}

\only<2>{\begin{textblock}{1}(9,2.7)\fontsize{14}{16}\sf
\begin{alertblock}{}\centerline{$\hat{y}_{t|t-1}$}\end{alertblock}\end{textblock}}
\only<2>{\begin{textblock}{.8}(10.5,2.7)\fontsize{14}{16}\sf
\begin{alertblock}{}\centerline{$\phantom{\hat{y}_{|}}{e}_{t}\phantom{\hat{y}_{|}}$}\end{alertblock}\end{textblock}}

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r dj4, echo=TRUE, warning=FALSE, fig.height=3.7}
augment(fit) %>%
  ggplot(aes(x = trading_day)) +
  geom_line(aes(y = Close, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted"))
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r dj4a, echo=TRUE, warning=FALSE, fig.height=3.7}
augment(fit) %>%
  filter(trading_day > 1100) %>%
  ggplot(aes(x = trading_day)) +
  geom_line(aes(y = Close, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted"))
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r dj5, echo=TRUE, warning = FALSE}
augment(fit) %>%
  autoplot(.resid) +
  labs(y = "$US",
       title = "Residuals from naïve method")
```

## Facebook closing stock price
\fontsize{11}{11}\sf

```{r dj6, warning=FALSE}
augment(fit) %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 150) +
  labs(title = "Histogram of residuals")
```

## Facebook closing stock price
\fontsize{11}{11}\sf

```{r dj7}
augment(fit) %>%
  ACF(.resid) %>%
  autoplot() + labs(title = "ACF of residuals")
```

## `gg_tsresiduals()` function
\fontsize{11}{12}\sf

```{r dj10, echo=TRUE, fig.height=4, warning = FALSE}
gg_tsresiduals(fit)
```


## ACF of residuals

  * We assume that the residuals are white noise (uncorrelated, mean zero, constant variance). If they aren't, then there is information left in  the residuals that should be used in computing forecasts.

  * So a standard residual diagnostic is to check the ACF of the residuals of a forecasting method.

  * We *expect* these to look like white noise.

## Portmanteau tests

Consider a *whole set* of $r_{k}$  values, and develop a test to see whether the set is significantly different from a zero set.\pause

\begin{block}{Box-Pierce test\phantom{g}}
\centerline{$\displaystyle
Q = T \sum_{k=1}^\ell r_k^2$}
where $\ell$  is max lag being considered and $T$ is number of observations.
\end{block}

  * If each $r_k$ close to zero, $Q$ will be **small**.
  * If some $r_k$ values large (positive or negative), $Q$ will be **large**.

## Portmanteau tests

Consider a *whole set* of $r_{k}$  values, and develop a test to see whether the set is significantly different from a zero set.

\begin{block}{Ljung-Box test}
\centerline{$\displaystyle
 Q^* = T(T+2) \sum_{k=1}^\ell (T-k)^{-1}r_k^2$}
where $\ell$  is max lag being considered and $T$ is number of observations.
\end{block}

  * My preferences: $\ell=10$ for non-seasonal data, $h=2m$ for seasonal data.
  * Better performance, especially in small samples.

\vspace*{10cm}

## Portmanteau tests
\fontsize{13}{15}\sf

  * If data are WN, $Q^*$ has $\chi^2$ distribution with  $(\ell - K)$ degrees of freedom where $K=$ no.\ parameters in model.
  * When applied to raw data, set $K=0$.
  * `lag` $= \ell$, `dof` $= K$

\fontsize{11}{12}\sf

```{r dj9, echo=TRUE}
augment(fit) %>%
  features(.resid, ljung_box, lag=10, dof=0)
```

<!-- ## Your turn

Compute seasonal naïve forecasts for quarterly Australian beer production from 1992.

\fontsize{10}{12}\sf

```{r, results = 'hide', fig.show='hide'}
recent <- aus_production %>% filter(year(Quarter) >= 1992)
fit <- recent %>% model(SNAIVE(Beer))
fit %>% forecast() %>% autoplot(recent)
```

\fontsize{14}{15}\sf

Test if the residuals are white noise.

\fontsize{10}{12}\sf

```{r, results = 'hide', fig.show='hide', warning = FALSE}
augment(fit) %>% features(.resid, ljung_box, lag=10, dof=0)
gg_tsresiduals(fit)
```

\fontsize{14}{15}\sf

What do you conclude? -->

# Distributional forecasts and prediction intervals

## Forecast distributions

 * A forecast $\hat{y}_{T+h|T}$ is (usually) the mean of the conditional distribution $y_{T+h} \mid y_1, \dots, y_{T}$.
 * Most time series models produce normally distributed forecasts.
 * The forecast distribution describes the probability of observing any future value.

## Forecast distributions

\fontsize{14}{18}\sf

Assuming residuals are normal, uncorrelated, sd = $\hat\sigma$:

\begin{block}{}
\begin{tabular}{ll}
\bf Mean: & $\hat{y}_{T+h|T} \sim N(\bar{y}, (1 + 1/T)\hat{\sigma}^2)$\\[0.2cm]
\bf Naïve: & $\hat{y}_{T+h|T} \sim N(y_T, h\hat{\sigma}^2)$\\[0.2cm]
\bf Seasonal naïve: & $\hat{y}_{T+h|T} \sim N(y_{T+h-m(k+1)}, (k+1)\hat{\sigma}^2)$\\[0.2cm]
\bf Drift: & $\hat{y}_{T+h|T} \sim N(y_T + \frac{h}{T-1}(y_T - y_1),h\frac{T+h}{T}\hat{\sigma}^2)$
\end{tabular}
\end{block}

where $k$ is the integer part of $(h-1)/m$.

Note that when $h=1$ and $T$ is large, these all give the same approximate forecast variance: $\hat{\sigma}^2$.

## Prediction intervals

 * A prediction interval gives a region within which we expect $y_{T+h}$ to lie with a specified probability.
 * Assuming forecast errors are normally distributed, then a 95% PI is
 \begin{alertblock}{}
\centerline{$
  \hat{y}_{T+h|T} \pm 1.96 \hat\sigma_h
$}
\end{alertblock}
where $\hat\sigma_h$ is the st dev of the $h$-step distribution.

 * When $h=1$, $\hat\sigma_h$ can be estimated from the residuals.

## Prediction intervals
\fontsize{10}{12}\sf

```{r brick-fc-interval, dependson='brick-fc'}
brick_fc %>% hilo(level = 95)
```

## Prediction intervals

 * Point forecasts often useless without a measure of uncertainty (such as prediction intervals).
 * Prediction intervals require a stochastic model (with random errors, etc).
 * For most models, prediction intervals get wider as the forecast horizon increases.
 * Use `level` argument to control coverage.
 * Check residual assumptions before believing them.
 * Usually too narrow due to unaccounted uncertainty.

# Forecasting with transformations

## Modelling with transformations

\fontsize{12}{13}\sf

```{r food, echo=TRUE, fig.height=3}
eggs <- prices %>%
  filter(!is.na(eggs)) %>% select(eggs)
eggs %>% autoplot() +
  labs(title="Annual egg prices",
       y="$US (adjusted for inflation)")
```


## Modelling with transformations

Transformations used in the left of the formula will be automatically back-transformed. To model log-transformed egg prices, you could use:

\fontsize{13}{15}\sf

```{r food-bt-fit, dependson='food'}
fit <- eggs %>%
  model(RW(log(eggs) ~ drift()))
fit
```

## Forecasting with transformations

\fontsize{11}{12}\sf

```{r food-bt-fc, dependson='food-bt-fit'}
fc <- fit %>%
  forecast(h = 50)
fc
```

## Forecasting with transformations
\fontsize{12}{13}\sf

```{r elec9,echo=TRUE,fig.height=3.5, dependson='food-bt-fc'}
fc %>% autoplot(eggs) +
  labs(title="Annual egg prices",
       y="US$ (adjusted for inflation)")
```

## Bias adjustment

  * Back-transformed point forecasts are medians.
  * Back-transformed PI have the correct coverage.

\pause

**Back-transformed means**

Let $X$ be have mean $\mu$ and variance $\sigma^2$.

Let $f(x)$ be back-transformation function, and $Y=f(X)$.

Taylor series expansion about $\mu$:
$$f(X) = f(\mu) + (X-\mu)f'(\mu) + \frac{1}{2}(X-\mu)^2f''(\mu).$$\pause

\begin{alertblock}{}
\centerline{$\E[Y] = \E[f(X)] = f(\mu) + \frac12 \sigma^2 f''(\mu)$}
\end{alertblock}

## Bias adjustment

\fontsize{13}{15}\sf

**Box-Cox back-transformation:**
\begin{align*}
y_t &= \left\{\begin{array}{ll}
        \exp(w_t)      & \quad \lambda = 0; \\
        (\lambda W_t+1)^{1/\lambda}  & \quad \lambda \ne 0.
\end{array}\right. \\
f(x) &= \begin{cases}
                        e^x & \quad\lambda=0;\\
 (\lambda x + 1)^{1/\lambda} & \quad\lambda\ne0.
 \end{cases}\\
f''(x) &= \begin{cases}
                        e^x & \quad\lambda=0;\\
 (1-\lambda)(\lambda x + 1)^{1/\lambda-2} & \quad\lambda\ne0.
 \end{cases}
\end{align*}\pause
\begin{alertblock}{}
\centerline{$\E[Y] = \begin{cases}
                        e^\mu\left[1+\frac{\sigma^2}{2}\right] & \quad\lambda=0;\\
 (\lambda \mu + 1)^{1/\lambda}\left[1+\frac{\sigma^2(1-\lambda)}{2(\lambda\mu+1)^2}\right] & \quad\lambda\ne0.
 \end{cases}$}
\end{alertblock}

## Bias adjustment
\fontsize{9}{9}\sf

```{r biasadj, fig.height=3.4, message=FALSE}
fc %>%
  autoplot(eggs, level = 80, point_forecast = lst(mean, median)) +
  labs(title="Annual egg prices",
       y="US$ (adjusted for inflation)")
```

# Forecasting and decomposition

## Forecasting and decomposition

\begin{block}{}
$$y_t = \hat{S}_t + \hat{A}_t$$
\vspace*{-0.5cm}\begin{itemize}\tightlist
  \item $\hat{A}_t$ is seasonally adjusted component
  \item $\hat{S}_t$ is seasonal component.
\end{itemize}
\end{block}

  *  Forecast $\hat{S}_t$ using SNAIVE.
  *  Forecast $\hat{A}_t$ using non-seasonal time series method.
  *  Combine forecasts of $\hat{S}_t$ and $\hat{A}_t$ to get forecasts of original data.

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail}
us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade") %>%
  select(-Series_ID)
us_retail_employment
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail1, echo=TRUE, fig.height=3.2}
dcmp <- us_retail_employment %>%
  model(STL(Employed)) %>%
  components() %>% select(-.model)
dcmp
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail2, echo=TRUE, fig.height=3.2}
dcmp %>%
  model(NAIVE(season_adjust)) %>%
  forecast() %>%
  autoplot(dcmp) +
  labs(title = "Naive forecasts of seasonally adjusted data")
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail3, echo=TRUE, fig.height=2.8}
us_retail_employment %>%
  model(stlf = decomposition_model(
    STL(Employed ~ trend(window = 7), robust = TRUE),
    NAIVE(season_adjust)
  )) %>%
  forecast() %>%
  autoplot(us_retail_employment)
```

\vspace*{10cm}

## Decomposition models

`decomposition_model()` creates a decomposition model

 * You must provide a method for forecasting the `season_adjust` series.
 * A seasonal naive method is used by default for the `seasonal` components.
 * The variances from both the seasonally adjusted and seasonal forecasts are combined.

# Evaluating forecast accuracy

## Training and test sets

```{r traintest, fig.height=1, echo=FALSE, cache=TRUE}
train <- 1:18
test <- 19:24
par(mar = c(0, 0, 0, 0))
plot(0, 0, xlim = c(0, 26), ylim = c(0, 2), xaxt = "n", yaxt = "n", bty = "n", xlab = "", ylab = "", type = "n")
arrows(0, 0.5, 25, 0.5, 0.05)
points(train, train * 0 + 0.5, pch = 19, col = "#0072B2")
points(test, test * 0 + 0.5, pch = 19, col = "#D55E00")
text(26, 0.5, "time")
text(10, 1, "Training data", col = "#0072B2")
text(21, 1, "Test data", col = "#D55E00")
```

-   A model which fits the training data well will not necessarily forecast well.
-   A perfect fit can always be obtained by using a model with enough parameters.
-   Over-fitting a model to data is just as bad as failing to identify a systematic pattern in the data.
  * The test set must not be used for *any* aspect of model development or calculation of forecasts.
  * Forecast accuracy is based only on the test set.

## Forecast errors

Forecast "error": the difference between an observed value and its forecast.
$$
  e_{T+h} = y_{T+h} - \hat{y}_{T+h|T},
$$
where the training data is given by $\{y_1,\dots,y_T\}$

- Unlike residuals, forecast errors on the test set involve multi-step forecasts.
- These are *true* forecast errors as the test data is not used in computing $\hat{y}_{T+h|T}$.

## Measures of forecast accuracy

```{r beer-fc-1, echo=FALSE, fig.height=4}
train <- aus_production %>%
  filter(between(year(Quarter), 1992, 2007))
beer <- aus_production %>%
  filter(year(Quarter) >= 1992)
beer_fc_plot <- train %>%
  model(
    Mean = MEAN(Beer),
    Naive = NAIVE(Beer),
    Seasonal_naive = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())
  ) %>%
  forecast(h=11) %>%
  autoplot(beer, level = NULL) +
    labs(title = "Forecasts for quarterly beer production",
         y = "Megalitres") +
    guides(colour=guide_legend(title="Forecast"))
beer_fc_plot
```

## Measures of forecast accuracy

\begin{tabular}{rl}
$y_{T+h}=$ & $(T+h)$th observation, $h=1,\dots,H$ \\
$\pred{y}{T+h}{T}=$ & its forecast based on data up to time $T$. \\
$e_{T+h} =$  & $y_{T+h} - \pred{y}{T+h}{T}$
\end{tabular}

\begin{align*}
\text{MAE} &= \text{mean}(|e_{T+h}|) \\[-0.2cm]
\text{MSE} &= \text{mean}(e_{T+h}^2) \qquad
&&\text{RMSE} &= \sqrt{\text{mean}(e_{T+h}^2)} \\[-0.1cm]
\text{MAPE} &= 100\text{mean}(|e_{T+h}|/ |y_{T+h}|)
\end{align*}\pause

  * MAE, MSE, RMSE are all scale dependent.
  * MAPE is scale independent but is only sensible if $y_t\gg 0$ for all $t$, and $y$ has a natural zero.

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = \text{mean}(|e_{T+h}|/Q)
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For non-seasonal time series,
$$
  Q = (T-1)^{-1}\sum_{t=2}^T |y_t-y_{t-1}|
$$
works well. Then MASE is equivalent to MAE relative to a naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = \text{mean}(|e_{T+h}|/Q)
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For seasonal time series,
$$
  Q = (T-m)^{-1}\sum_{t=m+1}^T |y_t-y_{t-m}|
$$
works well. Then MASE is equivalent to MAE relative to a seasonal naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

```{r beer-fc-2, echo=FALSE, fig.height=4}
beer_fc_plot
```

## Measures of forecast accuracy

\fontsize{12}{14}\sf

```{r beer-forecasts, results='hide'}
recent_production <- aus_production %>%
  filter(year(Quarter) >= 1992)
train <- recent_production %>%
  filter(year(Quarter) <= 2007)
beer_fit <- train %>%
  model(
    Mean = MEAN(Beer),
    Naive = NAIVE(Beer),
    Seasonal_naive = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())
  )
beer_fc <- beer_fit %>%
  forecast(h = 10)
```

## Measures of forecast accuracy
\fontsize{11}{11}\sf

```{r beer-train-accuracy, eval=FALSE}
accuracy(beer_fit)
```

\fontsize{9}{9}\sf

```{r beer-train-table, echo=FALSE}
accuracy(beer_fit) %>%
  arrange(.model) %>%
  select(.model, .type, RMSE, MAE, MAPE, MASE)
```

\fontsize{11}{11}\sf

```{r beer-test-accuracy, eval=FALSE}
accuracy(beer_fc, recent_production)
```

\fontsize{9}{9}\sf

```{r beer-test-table, echo=FALSE}
accuracy(beer_fc, recent_production) %>%
  arrange(.model) %>%
  select(.model, .type, RMSE, MAE, MAPE, MASE)
```

\vspace*{10cm}

<!-- ## Poll: true or false?


\begin{alertblock}{}\Large\bfseries\sffamily
https://pollev.com/robjhyndman
\end{alertblock}
 -->

# Time series cross-validation


```{r tscvplots, echo=FALSE}
tscv_plot <- function(.init, .step, h = 1) {
  expand.grid(
    time = seq(26),
    .id = seq(trunc(20 / .step))
  ) %>%
    group_by(.id) %>%
    mutate(
      observation = case_when(
        time <= ((.id - 1) * .step + .init) ~ "train",
        time %in% c((.id - 1) * .step + .init + h) ~ "test",
        TRUE ~ "unused"
      )
    ) %>%
    ungroup() %>%
    filter(.id <= 26 - .init) %>%
    ggplot(aes(x = time, y = .id)) +
    geom_segment(
      aes(x = 0, xend = 27, y = .id, yend = .id),
      arrow = arrow(length = unit(0.015, "npc")),
      col = "black", size = .25
    ) +
    geom_point(aes(col = observation), size = 2) +
    scale_y_reverse() +
    scale_color_manual(values = c(train = "#0072B2", test = "#D55E00", unused = "gray")) +
    #theme_void() +
    #geom_label(aes(x = 28.5, y = 1, label = "time")) +
    guides(col = FALSE) +
    labs(x="time", y="") +
    theme_void() +
    theme(axis.title = element_text())
}
```

## Time series cross-validation {-}

**Traditional evaluation**

```{r traintest1, fig.height=1, echo=FALSE, dependson="tscvplots"}
tscv_plot(.init = 18, .step = 20, h = 1:8) +
  geom_text(aes(x=10,y=0.8,label="Training data"), color = "#0072B2") +
  geom_text(aes(x=21,y=0.8,label="Test data"), color = "#D55E00") +
  ylim(1,0)
```

\pause

**Time series cross-validation**

```{r tscvggplot1, echo=FALSE}
tscv_plot(.init = 3, .step = 1, h = 1) +
  geom_text(aes(x=21,y=0,label="h = 1"), color = "#D55E00")
```


## Time series cross-validation {-}

**Traditional evaluation**

```{r traintest2, ref.label="traintest1", fig.height=1, echo=FALSE}
```

**Time series cross-validation**

```{r tscvggplot2, echo=FALSE,  dependson="tscvplots"}
tscv_plot(.init = 3, .step = 1, h = 2) +
  geom_text(aes(x=21,y=0,label="h = 2"), color = "#D55E00")
```

## Time series cross-validation {-}

**Traditional evaluation**

```{r traintest3, ref.label="traintest1", fig.height=1, echo=FALSE}
```

**Time series cross-validation**

```{r tscvggplot3, echo=FALSE,  dependson="tscvplots"}
tscv_plot(.init = 3, .step = 1, h = 3) +
  geom_text(aes(x=21,y=0,label="h = 3"), color = "#D55E00")
```


## Time series cross-validation {-}

**Traditional evaluation**

```{r traintest4, ref.label="traintest1", fig.height=1, echo=FALSE}
```

**Time series cross-validation**

```{r tscvggplot4, echo=FALSE,  dependson="tscvplots"}
tscv_plot(.init = 3, .step = 1, h = 4) +
  geom_text(aes(x=21,y=0,label="h = 4"), color = "#D55E00")
```

\only<2>{\begin{textblock}{9}(2,7)\begin{block}{}\fontsize{12}{13}\sf
\begin{itemize}\tightlist
\item Forecast accuracy averaged over test sets.
\item Also known as "evaluation on a rolling forecasting origin"
\end{itemize}\end{block}\end{textblock}}

\vspace*{10cm}

<!--
## Creating the rolling training sets {-}

\fontsize{13}{14}\sf

There are three main rolling types which can be used.

* Stretch: extends a growing length window with new data.
* Slide: shifts a fixed length window through the data.
* Tile: moves a fixed length window without overlap.

Three functions to roll a tsibble: `stretch_tsibble()`, `slide_tsibble()`,
and `tile_tsibble()`.

For time series cross-validation, stretching windows are most commonly used. -->

<!-- ## Creating the rolling training sets {-}

```{r animate, echo = FALSE, warning = FALSE, message = FALSE, fig.show='animate', interval=1/10, fig.height=4, fig.width=8, aniopts='controls,buttonsize=0.3cm,width=11.5cm'}
library(gganimate)
tourism_melb <- tourism %>%
  filter(Region == "Melbourne", Purpose == "Holiday") %>%
  select(Quarter, Trips)
slide_window <- slide_tsibble(tourism_melb, .size = 4) %>%
  mutate(type = "Slide") %>% as_tibble()
tile_window <- tile_tsibble(tourism_melb, .size = 4) %>%
  mutate(type="Tile")  %>% as_tibble()
tile_window <- bind_rows(
  tile_window %>% mutate(.id = 4*(.id-1)+1),
  tile_window %>% mutate(.id = 4*(.id-1)+2),
  tile_window %>% mutate(.id = 4*(.id-1)+3),
  tile_window %>% mutate(.id = 4*(.id-1)+4),
)
stretch_window <- stretch_tsibble(tourism_melb, .init = 4) %>%
  mutate(type = "Stretch") %>% as_tibble()
window <- bind_rows(slide_window, tile_window, stretch_window) %>%
  group_by(.id,type) %>%
  mutate(xmin = min(Quarter), xmax=max(Quarter), ymin=-Inf, ymax=Inf) %>%
  ungroup() %>%
  select(-Trips) %>%
  mutate(type = factor(type, levels=c("Stretch","Slide","Tile")))

ggplot() +
  geom_line(aes(x = Quarter, y = Trips), data = tourism_melb, colour = "grey", size = 1.2) +
  geom_rect(aes(
    xmin = xmin, xmax = xmax,
    ymin = ymin, ymax = ymax,
    group = .id
  ), data = window,
  fill = "#9ecae1", colour = "#9ecae1", size = 1.5, alpha = 0.6) +
  labs(x = "Quarter", y = "Trips") +
  facet_wrap(~ type, ncol = 1) +
  theme_bw() +
  transition_manual(.id)
```
 -->

## Time series cross-validation {-}

\fontsize{12}{13}\sf

Stretch with a minimum length of 3, growing by 1 each step.

```{r google-stretch, cache=TRUE}
fb_stretch <- fb_stock %>%
  stretch_tsibble(.init = 3, .step = 1) %>%
  filter(.id != max(.id))
```
\fontsize{10}{11}\sf
```{r google-stretch-print, echo = FALSE}
options(width = 60)
fb_stretch %>% select(Date, Close, trading_day, .id) %>% print(n=7)
```

## Time series cross-validation {-}

\small

Estimate RW w/ drift models for each window.

```{r google-fit, cache = TRUE}
fit_cv <- fb_stretch %>%
  model(RW(Close ~ drift()))
```

\fontsize{10}{11}\sf
```{r google-fit-print, echo = FALSE}
print(fit_cv, n = 4)
```

## Time series cross-validation {-}

\fontsize{9.5}{11}\sf

Produce one step ahead forecasts from all models.

```{r google-fc, cache = TRUE}
fc_cv <- fit_cv %>%
  forecast(h=1)
```

```{r google-fc-print, echo = FALSE}
fc_cv %>% select(-.model) %>% print(n = 4)
```

## Time series cross-validation {-}

\fontsize{11}{11}\sf

```{r google-accuracy, cache = TRUE, results = 'hide', eval = FALSE}
# Cross-validated
fc_cv %>% accuracy(fb_stock)
# Training set
fb_stock %>% model(RW(Close ~ drift())) %>% accuracy()
```

\fontsize{13}{15}\sf

```{r, echo = FALSE, warning = FALSE}
fc_cv %>% accuracy(fb_stock) %>%
  mutate(.type = "Cross-validation") %>%
  bind_rows(
    fb_stock %>%
      model(RW(Close ~ drift())) %>%
      accuracy()
  ) %>%
  transmute(Type = .type, RMSE, MAE, MAPE) %>%
  gt::gt("Type") %>%
  gt::fmt_number(columns=2:4, decimals=3) %>%
  gt::as_latex()
```

A good way to choose the best forecasting model is to find the model with the smallest RMSE computed using time series cross-validation.
